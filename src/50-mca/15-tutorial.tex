% !TEX root = main.tex

\section{Analyzing a program with LLVM-MCA}


\begin{frame}{Preface}
Let's look at how MCA allows us to predict the performance of some code \alert{without running it}\\
\medskip
Two examples:
\begin{itemize}
\item Sum reduction
\item Matrix multiplication
\end{itemize}
\medskip
We will look at:
\begin{itemize}
\item C language source code
\item Assembly language produced by the compiler
\item Analysis of the assembly language made by LLVM-MCA
\end{itemize}
We will learn about how to use LLVM-MCA and how to interpret its results as we go.
\end{frame}


\subsection{Basics of LLVM-MCA}


\begin{frame}{Sum Reduction}
\begin{block}{Algorithm}
Given a vector $V = [v_1, v_2, \ldots v_N]$ we compute:\\
\[
x = \sum_{i=1}^{N}{v_i}
\]
\end{block}
\begin{block}{Obvious Implementation}
\cinput{listings/01_add_reduction_v1_cleaned.c}
\end{block}
\begin{itemize}
\item Let's measure the performance we get with LLVM-MCA!
\end{itemize}
\end{frame}


\begin{frame}{Marking portions of code}
\begin{itemize}
\item We need to tell LLVM-MCA which portion of code we want to measure!
\end{itemize}
\bigskip
Important things to know:
\begin{enumerate}
\item branch instructions are ignored
\item every piece of code is assumed to be executed in a loop
\item the behavior of memory accesses is not simulated
\end{enumerate}
\bigskip
Consequences:
\begin{itemize}
\item We need to debug data locality issues \alert{first}
\item The only thing that can be analyzed feasibly is straight-line code
\end{itemize}
\end{frame}


\begin{frame}{Marking portions of code}
In this example there is not much possibility for data locality
issues (only sequential reads...)\\
\bigskip
From what we said it's clear that the portion of code we are interested
in is the \alert{body of the loop}\\
\medskip
To make LLVM-MCA analyze it we have to enclose it \alert{in the assembly code}
with:\\
\smallskip
\begin{center}
\large
\texttt{\# LLVM-MCA-BEGIN }\textit{block\_name}\\
\smallskip
\texttt{\# LLVM-MCA-END }\textit{block\_name}\\
\medskip
{\footnotesize \textit{block\_name} is any string identifier you want}
\end{center}
\end{frame}


\begin{frame}{Marking portions of code}
If we are working in the C language, we can use \texttt{asm} blocks:
\begin{block}{Obvious Implementation (annotated)}
\cinput[\tt\small]{listings/01_add_reduction_v1.c}
\end{block}
\bigskip
\alert{Warning:} Adding \texttt{asm} blocks can prevent optimizations!\\
\smallskip
{\footnotesize In this particular case, it prevents \alert{loop unrolling}.
For the purpose of the example it's not a problem because we will talk about
loop unrolling in a bit...}
\end{frame}


\begin{frame}{Marking portions of code}
Let's compile with \texttt{-O3}...
\begin{block}{Obvious Implementation (compiled)}
\asminput[\tt\small]{listings/01_add_reduction_v1.s}
\end{block}
\bigskip
\end{frame}


\begin{frame}{Running LLVM-MCA}
It's now time to actually run the analysis with LLVM-MCA.\\
\medskip
There are several kinds of options available:
\begin{itemize}
\item Machine type options (defaults autodetected from host)
\smallskip
\item Analysis configuration options
	{\small
	\begin{description}[\texttt{-bottleneck-analysis}]
	\item[\texttt{-iterations}] Number of iterations to simulate
	\item[\texttt{-register-file-size}] Size of the shadow register file
	\item[\texttt{-bottleneck-analysis}] Enables bottleneck analysis
	\end{description}
	\smallskip
	}
\item Options for toggling \alert{views} of the analysis data:
	{\small
	\begin{description}[\texttt{-instruction-tables}]
	\item[\texttt{-resource-pressure}] Enable the resource pressure view.
	\item[\texttt{-instruction-info}] Enable the instruction info view.
	\item[\texttt{-instruction-tables}] Prints resource pressure information based on the static information available from the processor model.
	\end{description}
	}
\end{itemize}
\end{frame}


\begin{frame}{Running LLVM-MCA}
We'll use the following options:
\begin{itemize}
\item \texttt{-timeline}
\item \texttt{-all-stats}
\item \texttt{-bottleneck-analysis}
\end{itemize}
\medskip
The command line syntax is very simple.
\begin{block}{LLVM-MCA invocation command}
\tt \$ llvm-mca add\_reduction.c \textbackslash \\
\hphantom{MMMM}-timeline -all-stats -bottleneck-analysis
\end{block}
\end{frame}


\begin{frame}{The Views of LLVM-MCA}
The output is partitioned for each \alert{code region} and for each \alert{data view}.\\
\smallskip
In our example there is just one region.\\
\medskip
The main view presents generic information about the simulation.
%
\begin{columns}[onlytextwidth]
%
\column{0.45\textwidth}
\begin{block}{The Main View}
\txtinput[\tt\small]{listings/01_add_reduction_v1_p01.txt}
\end{block}
%
\column{0.5\textwidth}
\begin{description}[ABC]
\item[\texttt{Iterations}:] Number of iterations of the loop that have been simulated.
\item[\texttt{Instructions}:] Total number of instructions simulated.
\item[\texttt{Total Cycles}:] Number of clock cycles taken for performing the code.
\item[\texttt{Total uOps}:] Number of CPU microcode instructions issued.
\end{description}
%
\end{columns}
%
\end{frame}


\begin{frame}{The Views of LLVM-MCA}
\begin{columns}[onlytextwidth]
%
\column{0.45\textwidth}
\begin{block}{The Main View}
\txtinput[\tt\small]{listings/01_add_reduction_v1_p01.txt}
\end{block}
%
\column{0.5\textwidth}
\begin{description}[ABC]
\item[\texttt{Dispatch Width}:] Maximum number of microinstructions that can be issued simultaneously
\item[\texttt{uOps Per Cycle}:] Average number of microinstructions executed per clock cycle.
\item[\texttt{IPC}:] Average number of instructions executed per clock cycle.
\item[\texttt{Block RThroughput}:] Average \alert{reciprocal throughput} of the code block
\end{description}
%
\end{columns}
%
\end{frame}


\begin{frame}{Reciprocal Throughput}
\begin{center}
The \alert{throughput} is the \emph{amount of work done in a given time}\\
\medskip
The \alert{reciprocal throughput} is \emph{how much time is required to perform the unit of work}\\
%
\bigskip
\begin{tabular}{>{\raggedleft}p{0.4\textwidth} c p{0.4\textwidth}}
\emph{unit of work} & = & one iteration of the loop\\
\emph{time} & = & clock cycles \\
\end{tabular}\\
\bigskip
%
The reciprocal throughput given by LLVM-MCA is \alert{theoretical}.\\
It does not take into account data dependencies amongst multiple iterations.\\
\medskip
It depends from the resource usage of the block of code.
\end{center}
\end{frame}


\begin{frame}{Reciprocal Throughput}
\begin{center}
The IPC (Instructions per Cycle) estimation \alert{does} take into account data dependencies.\\
\bigskip
Since IPC (like MIPS) is a measure of \alert{throughput}, we can derive the \alert{maximum throughput} by computing the \alert{inverse of the RThroughput}\\
\bigskip
\[
\textsf{IPC} \le \frac{N_{block}}{\textsf{Block RThroughput}}
\]
\[
N_{block} = \frac{\textsf{Instructions}}{\textsf{Iterations}}
\]
\end{center}
\end{frame}


\begin{frame}{Data dependencies are important!}
Let's try calculating the maximum IPC for our example...
\[
\textsf{IPC}_{max} = \frac{N_{block}}{\textsf{Block RThroughput}} = \frac{1}{0.5} = 2
\]
The maximum IPC is much higher than the \alert{actual} IPC ($0.25$)!\\
\medskip
This is because \alert{every instruction executed depends on the result of the previous one}
\end{frame}


\begin{frame}{The Timeline View}
We can verify visually if an instruction is waiting for another one using the \alert{timeline view}.
\bigskip
\begin{block}{Timeline View}
\txtinput[\tt\fontsize{6.7pt}{8pt}\selectfont]{listings/01_add_reduction_v1_p02.txt}
\end{block}
\end{frame}


\begin{frame}{The Timeline View}
\begin{block}{Timeline View}
\txtinput[\tt\fontsize{6.7pt}{8pt}\selectfont]{listings/01_add_reduction_v1_p02.txt}
\end{block}
On the \emph{left} the \alert{index} column contains the loop iteration counter and the index of the instruction in the block.\\
\medskip
On the \emph{right} the instructions are shown.\\
\medskip
In the \emph{middle} there is a \alert{timing graph} showing in which stage the instruction is in at each CPU cycle (from left to right). 
\end{frame}


\begin{frame}{The Timeline View}
\begin{block}{Timeline View}
\txtinput[\tt\fontsize{6.7pt}{8pt}\selectfont]{listings/01_add_reduction_v1_p02.txt}
\end{block}
\begin{description}
\item[\texttt{D}] The instruction is \alert{dispatched}\\(allocated to a reservation station)
\item[\texttt{e}] The instruction is being \alert{executed}
\item[\texttt{E}] The instruction \alert{finishes execution}, the result is ready
\item[\texttt{R}] The instruction is \alert{retired}\\(the reservation station is freed)
\end{description}
\end{frame}


\begin{frame}{The Instruction Info View}
\begin{block}{Instruction Info View}
\txtinput[\tt\fontsize{6pt}{7pt}\selectfont]{listings/01_add_reduction_v1_p03.txt}
\end{block}
\begin{itemize}
\item The dispatch width we saw earlier is \alert{6}
\item In the timeline view, at most \alert{3} instructions are dispatched together
\item In fact, can see in the \alert{instruction info view} that our instructions take 2 microinstructions to execute!
\end{itemize}
\end{frame}


\begin{frame}{The Instruction Info View}
\begin{block}{Instruction Info View}
\txtinput[\tt\fontsize{6pt}{7pt}\selectfont]{listings/01_add_reduction_v1_p03.txt}
\end{block}
Other interesting information shown here:
\begin{description}[\texttt{RThroughput}]
\item[\texttt{Latency}] Number of cycles required to execute the instruction
\item[\texttt{RThroughput}] Reciprocal of the number of such instructions that can execute simultaneously
\end{description}
\end{frame}


\begin{frame}{The Timeline and Microinstructions}
\begin{block}{Timeline View}
\txtinput[\tt\fontsize{6.7pt}{8pt}\selectfont]{listings/01_add_reduction_v1_p02.txt}
\end{block}
These two microinstructions (presumably!) perform, in order:
\begin{enumerate}
\item the \alert{load from memory} of the next number to accumulate
\item the \alert{sum} of the loaded number and the value in the register
\end{enumerate}
This is why the executions partially overlap --- only the \alert{sum} microinstruction is blocking.
\end{frame}


\begin{frame}{The Bottleneck Analysis}
\begin{overprint}
\onslide<1>
LLVM-MCA is able to automatically digest all this information for us, producing an analysis of the instruction sequence which has the biggest impact on the execution.
\onslide<2>
We can see right away that there is a data dependency on the \texttt{\%xmm0} register.
\end{overprint}
\begin{block}{Bottleneck View}
\txtinput[\tt\fontsize{5.7pt}{6pt}\selectfont]{listings/01_add_reduction_v1_p04.txt}
\end{block}
\end{frame}


\subsection{Effects of Loop Unrolling}


\begin{frame}{Hold on a sec, you cheater bastard!}
\begin{overprint}
\onslide<1>
Wait... weren't there \alert{some other instructions in the loop} apart from the sum!?
\onslide<2>
The results we looked at must have been completely wrong!
\end{overprint}
\begin{block}{Exhibit A}
\asminput[\tt\small]{listings/01_add_reduction_v1.s}
\end{block}
\end{frame}


\begin{frame}{I plead guilty!}
I surrender! You are right! I apologize! Let's annotate all the instructions in the loop!
\begin{block}{Remedial actions}
\asminput[\tt\small]{listings/01_add_reduction_v1b.s}
\end{block}
\end{frame}


\begin{frame}{I plead guilty!}
\begin{center}
Let's look at what's changed:
%
\begin{columns}

\column{0.45\textwidth}
\begin{block}{Before}
\asminput[\tt\small]{listings/01_add_reduction_v1_p01.txt}
\end{block}

\column{0.45\textwidth}
\begin{block}{After}
\asminput[\tt\small]{listings/01_add_reduction_v1b_p01.txt}
\end{block}

\end{columns}
\medskip
IPC, \emph{uOps Per Cycle} and \emph{Block RThroughput} are much worse!\\
\medskip
{\footnotesize Mmmh, I wonder why the total number of cycles has stayed the same... Must be a bug.}
\end{center}
\end{frame}


\begin{frame}{I plead guilty!}
\begin{center}
\begin{block}{Bottleneck View}
\txtinput[\tt\fontsize{5.7pt}{6pt}\selectfont]{listings/01_add_reduction_v1b_p04.txt}
\end{block}
\medskip
\begin{overprint}
\onslide<1>
The bottleneck view unambiguosly says that the biggest issue here is the update of the induction variable of the loop!
\onslide<2>
The perfect cure to this horrible disease is a classic and ever-lasting optimization: \alert{loop unrolling}!
\end{overprint}
\end{center}
\end{frame}


\begin{frame}{First Performance Improvement}
\begin{block}{Now we're talking!}
\asminput[\tt\small]{listings/01_add_reduction_v2.s}
\end{block}
\end{frame}


\begin{frame}{First Performance Improvement}
Let's run the old and the new modified program side by side to show to the world that I am the best programmer ever!\footnote{All experiments shown here were performed on a Intel Core i5-8259U CPU (Skylake microarchitecture) running at 2.30 GHz (peak 3.60 GHz).}
\begin{block}{Speedup Speedup Speedup}
\asminput[\tt\small]{listings/01_add_reduction_v1_vs_v2_spdup.txt}
\end{block}
\pause
\alert{\centering WHAAAAAT!? The improvement is MINIMAL!\\}
\end{frame}


\begin{frame}{A closer look at the data}
\begin{block}{Timeline Comparison}
\asminput[\tt\fontsize{6.7pt}{7pt}\selectfont]{listings/01_add_reduction_v1_vs_v2_timeline.txt}
\end{block}
If we look at the timeline we can clearly see that LLVM-MCA \alert{correctly predicted} that the loop condition did not affect the sum operations, even without loop unrolling. 
\end{frame}


\begin{frame}{A closer look at the data}
\begin{columns}

\column{0.45\textwidth}
\begin{block}{Before}
\asminput[\tt\small]{listings/01_add_reduction_v1_p01.txt}
\end{block}

\column{0.45\textwidth}
\begin{block}{After}
\asminput[\tt\small]{listings/01_add_reduction_v1b_p01.txt}
\end{block}

\end{columns}
\medskip
In fact, the ratio between the total number of CPU cycles and the number of iterations was the same as well, regardless of whether we were including the loop condition.
\end{frame}


\begin{frame}{A closer look at the data}
\begin{columns}

\column{0.45\textwidth}
\begin{block}{Before}
\asminput[\tt\small]{listings/01_add_reduction_v1b_p01.txt}
\end{block}

\column{0.45\textwidth}
\begin{block}{After}
\asminput[\tt\small]{listings/01_add_reduction_v2_p01.txt}
\end{block}

\end{columns}
\medskip
The ratio between the total number of CPU cycles and the number of additions performed is the same even between the version with loop unrolling and the version without\\
\smallskip
{\footnotesize With loop unrolling each iteration performs 8 additions}
\end{frame}


\begin{frame}{The Resource Pressure View}
The reason why the loop condition computation can run in parallel with respect to the sum is that they require \alert{different CPU resources}.\\
\smallskip
We can see in detail the pressure on each resource from the \alert{Resource Pressure View}.
\bigskip
\begin{block}{Resource Pressure View}
\txtinput[\tt\fontsize{5.5pt}{6pt}\selectfont]{listings/01_add_reduction_v1b_p05.txt}
\end{block}
\end{frame}


\begin{frame}{The Resource Pressure View}
\begin{block}{Resource Pressure View}
\txtinput[\tt\fontsize{5.5pt}{6pt}\selectfont]{listings/01_add_reduction_v1b_p05.txt}
\end{block}
For each resource, this view shows the average number of resource cycles consumed at every iteration by the instructions.\\
\medskip
The concept of resources roughly corresponds to \alert{reservation stations} and the number and type of resources depends on the microarchitecture which is target of analysis.
\end{frame}


\begin{frame}{The Resource Pressure View}

\begin{columns}

\column{0.45\textwidth}
\begin{block}{Resource Pressure View: List of Resources}
\txtinput{listings/list-of-resources.txt}
\end{block}

\column{0.45\textwidth}

\begin{overprint}
\onslide<1>
\raggedright\bigskip
In our example, the CPU microarchitecture is \alert{Intel Skylake}.\\
We have \alert{7 \texttt{SKLPort} resources}.
\begin{itemize}
\item \emph{SKL} is a contraction of \emph{SKyLake}
\item \emph{Port} means \emph{Reservation Station} in Intel's terminology
\end{itemize}
Each one of these resources serves different Functional Units.
\vfill
%
\onslide<2>
\begin{description}[\texttt{SKLPortM}]
\item[\texttt{SKLPort0}] Integer/float vector/scalar ALU, integer mul. and div., branch
\item[\texttt{SKLPort1}] Integer/float vector/scalar ALU
\item[\texttt{SKLPort2}] Load
\item[\texttt{SKLPort3}] Load
\item[\texttt{SKLPort4}] Store
\item[\texttt{SKLPort5}] Integer vector/scalar ALU
\item[\texttt{SKLPort6}] Integer scalar, branch
\item[\texttt{SKLPort7}] Store address
\end{description}
%
\onslide<3>
\raggedright\bigskip
The \texttt{SKLDivider} and \texttt{SKLFPDivider} resources are 
\alert{dummy resources}\\
\bigskip
They are used by the LLVM model to represent some complex
behavior that cannot be represented with a simulation based on simple pipelining.\\
\bigskip
Hardware-wise they don't exist.
\end{overprint}

\end{columns}

\end{frame}


\begin{frame}{A closer look at the data}
We can attempt to \alert{predict the execution time} using the total cycles / iteration count ratio.\\
\smallskip
Given that the actual program we are benchmarking executes $80\times10^{6}$ iterations:
\[
T_{exc} = \frac{N_{cycles}}{f_{ck}}
\]
\[
N_{cycles} = \frac{\textsf{Total Cycles}}{\textsf{Iterations}}80\times10^{6} \approx \frac{400}{100}80\times10^{6} = 320\times10^{6}
\]
\[
T_{exc} = \frac{320\times10^{6}}{3.6\times10^9 \si{\second}^{-1}} \approx \SI{0.088889}{\second}
\]
\end{frame}


\begin{frame}{A closer look at the data}
We measured an execution time of $0.10$ seconds, while LLVM-MCA estimates an execution time of $0.089$ seconds.
\bigskip
\begin{itemize}
\item The prediction by LLVM-MCA is close but not identical to the real value
\item The discrepancy can be attributed by the fact that LLVM-MCA does not simulate exactly every detail of the CPU, \emph{especially memory behavior}
	\begin{itemize}
	\item In fact, if we modify the program to always read from the same array element,
	the execution time drops to $\approx \SI{0.093175}{\second}$
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{What does this tell us?}
\large
\begin{enumerate}
\item Loop unrolling had a relatively insignificant effect because \emph{the loop condition and branching instructions use different CPU resources than FPU instructions}
\bigskip
\item Traditional measurements of performance based on the instruction as the unit of work \emph{are deceptive and useless}
	\medskip
	\begin{itemize}
	\item Corollary: The performance numbers given by many CPU benchmarks are completely bogus and easy to manipulate
	\item The problem is that not every instruction performs the same amount of work!
	\end{itemize}
\end{enumerate}
\end{frame}


